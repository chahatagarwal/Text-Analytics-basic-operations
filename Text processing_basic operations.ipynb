{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\srisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\srisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\srisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#install dependencies for NLTK - stopwords & wordnet\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#additional data pre-processing - \\t related\n",
    "def addl_clean_word(word):\n",
    "    word = word.replace('can\\'t','cannot')\n",
    "    word = word.replace('won\\'t','would not')\n",
    "    word = word.replace('doesn\\'t','does not')\n",
    "    word = word.replace('wouldn\\'t','would not')\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data pre-processing phase\n",
    "\n",
    "#import necessary libraries and packages\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re \n",
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "#function to clean unwanted contents as a phase of pre-processing\n",
    "def pre_processing(data_for_pre_processing):\n",
    "    #remove unwanted contents: remove other characters apart from english alphabets (i.e; Numbers, extra space, special characters, punctuation, single letters\n",
    "    UWC_RE = re.compile(r'[.|,|)|(|\\|/|?|!|\"|#|+|-|:|_|=|;|~|`|@|$|%|&|*|{|}|!]', re.IGNORECASE)\n",
    "\n",
    "    #remove html contents\n",
    "    HTML_RE = re.compile(r'\\b<.*?>\\b')\n",
    "\n",
    "    #remove single characters\n",
    "    S_RE = re.compile(r'\\s+[a-zA-Z]\\s+')\n",
    "    \n",
    "    #remove multiple white spaces\n",
    "    W_RE = re.compile(r'\\s+')\n",
    "\n",
    "    #remove digits\n",
    "    D_RE = re.compile(r'[0-9]')\n",
    "\n",
    "    #object for lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    \n",
    "    #lower-case conversion\n",
    "    data_for_pre_processing = data_for_pre_processing.lower()\n",
    "    \n",
    "    #additional cleaning of data\n",
    "    data_for_pre_processing = addl_clean_word(data_for_pre_processing)\n",
    "    \n",
    "    #removal of HTML content\n",
    "    data_for_pre_processing = HTML_RE.sub('', data_for_pre_processing)\n",
    "    \n",
    "    #removal of unwanted content\n",
    "    data_for_pre_processing = UWC_RE.sub('', data_for_pre_processing.strip())\n",
    "    \n",
    "    #removal of Digits\n",
    "    data_for_pre_processing = D_RE.sub('', data_for_pre_processing.strip())\n",
    "    \n",
    "    #removal of Single characters\n",
    "    data_for_pre_processing = S_RE.sub('', data_for_pre_processing.strip())\n",
    "    \n",
    "    #removal of extra white space\n",
    "    data_for_pre_processing = W_RE.sub(' ', data_for_pre_processing.strip())\n",
    "    \n",
    "    #split into words\n",
    "    data_for_pre_processing = word_tokenize(data_for_pre_processing) \n",
    "    \n",
    "    #lemmatize the words\n",
    "    data_for_pre_processing = [lemmatizer.lemmatize(word.strip()) for word in data_for_pre_processing if not word in (set(stopwords.words('english'))) or (word==\"not\")]\n",
    "    \n",
    "    #make back into sentence\n",
    "    data_for_pre_processing_final = ' '.join(data_for_pre_processing)\n",
    "    \n",
    "    return data_for_pre_processing_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data cleaning phase\n",
    "import re\n",
    "\n",
    "#function for Data Cleaning -> data is text or mail body and flag takes 0 or 1 depending upon the mail trails\n",
    "def mail_body_process(data,flag):\n",
    "    # regex to catch header lines\n",
    "    email_component = re.compile(r'((From:|Sent:|To:|Cc:|Subject:|Date:|RE:).*)', re.IGNORECASE)\n",
    "\n",
    "    # regex to catch lines with contact details\n",
    "    contact_component = re.compile(r'(Phone.*:)|(Cell:.*)|(Email:.*)|(M:.*)|(Merchant Number:.*)|(ATTENTION:.*)|(Escalation:.*)|(USER:.*)|(WARNING:.*)|(Website:.*)|(NB:.*)|(Number:.*)|(ref:.*)|(Disclaimer and confidentiality note:.*)', re.IGNORECASE)\n",
    "\n",
    "    #backup the mail-body \n",
    "    backup_data = data\n",
    "    #spliting string based on new line\n",
    "    remove = data.split(\"\\n\")\n",
    "    data_after_removal = \"\"\n",
    "    #loop over the number of lines\n",
    "    for m in range(flag,len(remove)): \n",
    "        #eliminate white spaces\n",
    "        remove[m] = remove[m].strip().replace(r'\\n',\"\").replace(r'\\r',\"\")\n",
    "        if(remove[m].strip()):\n",
    "            if (email_component.findall(remove[m].strip())):\n",
    "                continue\n",
    "            if (contact_component.findall(remove[m].strip())):\n",
    "                continue\n",
    "            #process the data and also remove stoplist of SBSA\n",
    "            else: \n",
    "                data_after_removal = data_after_removal + \"\".join(TAG_RE.sub(' ', remove[m].strip())) + \" \"\n",
    "    if(len(data_after_removal)):           \n",
    "        #data pre-processing step\n",
    "        data_after_removal = pre_processing(data_after_removal)\n",
    "        return data_after_removal \n",
    "    else:\n",
    "        return backup_data         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
